{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the number of rows by random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the number of rows we are selecting random indexs of the original data frame and deleting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the number of rows to 10000\n",
    "df = pd.read_csv('bank-additional-full.csv',  sep=';', na_values=\".\")\n",
    "nRowsToRemove = len(df) - 10000\n",
    "rowsToDropIndices = np.random.choice(df.index, nRowsToRemove, replace = False)\n",
    "df_reduced = df.drop(rowsToDropIndices)\n",
    "df_reduced.to_csv(r'bank-additional-reduced.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study if the result is representative\n",
    "After doing a sample of the original data set we need to test that the result is representative and that for each column we have a similar statistical structure to\n",
    "the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of all the features \n",
    "With these descriptions we can already observe for the numerical variables if the mean, std, min and max values or others have changed or not. If the values are very similar, we can accept the previous algorithm to reduce the number of rows and proceed with the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study of the  proportions/frequencies of each feature in the original and row-reduced data set\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate that the categorical variables values have the same representation in both the original and the reduced data sets is not enough to observe the previous description. We must observe that the percentage of each feature value has not changed much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Job feature percentatges in the original data set\")\n",
    "print(df.job.value_counts(normalize=True))\n",
    "print (\"Job feature percentatges in the reduced data set\")\n",
    "print(df_reduced.job.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'job' value in the original data set and the reduced data set\")\n",
    "print(df.job.value_counts(normalize=True)-df_reduced.job.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'marital' value in the original data set and the reduced data set\")\n",
    "print(df.marital.value_counts(normalize=True)-df_reduced.marital.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'education' value in the original data set and the reduced data set\")\n",
    "print(df.education.value_counts(normalize=True)-df_reduced.education.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'default' value in the original data set and the reduced data set\")\n",
    "print(df.default.value_counts(normalize=True)-df_reduced.default.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'housing' value in the original data set and the reduced data set\")\n",
    "print(df.housing.value_counts(normalize=True)-df_reduced.housing.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'loan' value in the original data set and the reduced data set\")\n",
    "print(df.loan.value_counts(normalize=True)-df_reduced.loan.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'contact' value in the original data set and the reduced data set\")\n",
    "print(df.contact.value_counts(normalize=True)-df_reduced.contact.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'month' value in the original data set and the reduced data set\")\n",
    "print(df.month.value_counts(normalize=True)-df_reduced.month.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'day_of_week' value in the original data set and the reduced data set\")\n",
    "print(df.day_of_week.value_counts(normalize=True)-df_reduced.day_of_week.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'poutcome' value in the original data set and the reduced data set\")\n",
    "print(df.poutcome.value_counts(normalize=True)-df_reduced.poutcome.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'y' value in the original data set and the reduced data set\")\n",
    "print(df.y.value_counts(normalize=True)-df_reduced.y.value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the sample obtained is representative and we can use it for our project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the number of rows to balance the classes\n",
    "The previous reduction of rows does not take into account if the dataset is balanced or not. As our data set is not balanced, we  can reduce the data set by eliminating only rows of the class with bigger proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.y.value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the rows with y=no represent the 88.7% (36548 rows) of the totality and rows with y=yes represent only the 11.26% (4640). Our data set is very  unbalanced so, to solve this problem, we can eliminate rows that have y=no until having 10k in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the number of rows to 10000\n",
    "nRowsToRemove = len(df) - 10000 #we could higher the number of rows to remove to end with exactly the same proportion of yes and no (9280 rows in total)\n",
    "rowsToDropIndices = np.random.choice(df[df.y == 'no'].index, nRowsToRemove, replace = False)\n",
    "df_balanced = df.drop(rowsToDropIndices)\n",
    "df_balanced.to_csv(r'bank-additional-reduced-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study of the  proportions/frequencies of each feature in the original and row-reduced data set\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study of the  proportions/frequencies of each feature in the original and row-reduced data set\n",
    "df_balanced.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'job' value in the original data set and the balanced data set\")\n",
    "print(df.job.value_counts(normalize=True)-df_balanced.job.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'marital' value in the original data set and the balanced data set\")\n",
    "print(df.marital.value_counts(normalize=True)-df_balanced.marital.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'education' value in the original data set and the balanced data set\")\n",
    "print(df.education.value_counts(normalize=True)-df_balanced.education.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'default' value in the original data set and the balanced data set\")\n",
    "print(df.default.value_counts(normalize=True)-df_balanced.default.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'housing' value in the original data set and the balanced data set\")\n",
    "print(df.housing.value_counts(normalize=True)-df_balanced.housing.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'loan' value in the original data set and the balanced data set\")\n",
    "print(df.loan.value_counts(normalize=True)-df_balanced.loan.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'contact' value in the original data set and the balanced data set\")\n",
    "print(df.contact.value_counts(normalize=True)-df_balanced.contact.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'month' value in the original data set and the balanced data set\")\n",
    "print(df.month.value_counts(normalize=True)-df_balanced.month.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'day_of_week' value in the original data set and the balanced data set\")\n",
    "print(df.day_of_week.value_counts(normalize=True)-df_balanced.day_of_week.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Difference between the percentage of each 'poutcome' value in the original data set and the balanced data set\")\n",
    "print(df.poutcome.value_counts(normalize=True)-df_balanced.poutcome.value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution now statistics like the means, std, frequencies and other, have changed a bit more than they did in the previous form of reducing. the number of rows studied. One problem is that for the feature \"default\" we are losing the modality 'yes'. This modality in the original dataset had pretty low representation, exactly only 3 rows had default = 'yes' value.\n",
    "Despite that, all the other statistical changes are not that drastic.\n",
    "\n",
    "Although we have now a balanced dataset, we should consider other ways to balance the data set and pick that one that is a better sample and represents better the original data set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "979bf14ea64443ff3ffb738d52926696eb30c1bf62b3b549289c26deae58448a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
